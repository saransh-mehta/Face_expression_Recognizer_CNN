{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we ll focus on retriving image from csv column\n",
    "# the values are stored in a string in image column\n",
    "# so, we ll first extract out values and put into array, convert it to numpy array\n",
    "# and then resize into (48,48)\n",
    "# then append one by one all rows into an array, convert the bigger array to numpy\n",
    "\n",
    "df = pd.read_csv(os.path.abspath('fer2013') + '/fer2013.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFromStr(df):\n",
    "    \n",
    "    imgArray = []\n",
    "    \n",
    "    for img in df['pixels']:\n",
    "        numberArray = []\n",
    "        \n",
    "        #print(img.split(' '))\n",
    "        for number in img.split(' '):\n",
    "            numberArray.append(number)\n",
    "            \n",
    "        numberArray = np.array(numberArray, dtype = np.int)\n",
    "        numberArray = np.reshape(numberArray, (48, 48, 1))\n",
    "        \n",
    "        imgArray.append(numberArray)\n",
    "        \n",
    "    imgArray = np.array(imgArray)\n",
    "    print(imgArray.shape)\n",
    "    \n",
    "    return imgArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "allImgArray = extractFromStr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we ll bring up the labels too\n",
    "\n",
    "labels = np.array(df['emotion'])\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we ll shuffle the data with images and labels together\n",
    "\n",
    "data, labelShuffle = shuffle(allImgArray, labels, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nji9Egfw/k2pxt6FMuqsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc35c221b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let us see the image along with its label\n",
    "\n",
    "imgId = 3\n",
    "\n",
    "imgPlot = np.reshape(data[imgId], (48, 48))\n",
    "\n",
    "plt.imshow(imgPlot, cmap = 'gray')\n",
    "#plt.xlabel(labelShuffle[imgId])\n",
    "print(labelShuffle[imgId])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we ll go into building cnn\n",
    "\n",
    "#first choosing the hyper parameters\n",
    "batchSize = 512\n",
    "epochs = 30 # we ll do in total 300 epochs, in gropus of 30, as model architecture is too big\n",
    "classes = 7\n",
    "\n",
    "# 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
    "\n",
    "# now we ll convert labels into one hot encodding\n",
    "\n",
    "labelsOneHot = np_utils.to_categorical(labelShuffle, classes)\n",
    "labelsOneHot[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# here we ll use zero padding to keep the shape same,\n",
    "# in the first layer we need to give input shape\n",
    "\n",
    "model.add(Convolution2D(filters = 32, kernel_size = (3, 3), padding = 'same', input_shape = (48, 48, 1)))\n",
    "convout1 = Activation('relu')\n",
    "model.add(convout1)\n",
    "\n",
    "model.add(Convolution2D(filters = 32, kernel_size = (3, 3), padding = 'same'))\n",
    "convout2 = Activation('relu')\n",
    "model.add(convout2)\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(filters = 64, kernel_size = (3, 3), padding = 'same'))\n",
    "convout3 = Activation('relu')\n",
    "model.add(convout3)\n",
    "\n",
    "model.add(Convolution2D(filters = 64, kernel_size = (3, 3), padding = 'same'))\n",
    "convout4 = Activation('relu')\n",
    "model.add(convout4)\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(filters = 128, kernel_size = (3, 3), padding = 'same'))\n",
    "convout5 = Activation('relu')\n",
    "model.add(convout5)\n",
    "\n",
    "model.add(Convolution2D(filters = 128, kernel_size = (3, 3), padding = 'same'))\n",
    "convout6 = Activation('relu')\n",
    "model.add(convout6)\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "            \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 83s - loss: 2.3388 - acc: 0.2396 - val_loss: 1.7140 - val_acc: 0.3054\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 60s - loss: 1.6493 - acc: 0.3518 - val_loss: 1.5740 - val_acc: 0.3945\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 60s - loss: 1.5111 - acc: 0.4177 - val_loss: 1.5025 - val_acc: 0.4202\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 60s - loss: 1.3943 - acc: 0.4657 - val_loss: 1.3474 - val_acc: 0.4865\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 61s - loss: 1.3007 - acc: 0.5053 - val_loss: 1.2948 - val_acc: 0.5065\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 58s - loss: 1.2239 - acc: 0.5389 - val_loss: 1.2475 - val_acc: 0.5252\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 58s - loss: 1.1318 - acc: 0.5718 - val_loss: 1.2039 - val_acc: 0.5458\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 58s - loss: 1.0620 - acc: 0.5978 - val_loss: 1.2218 - val_acc: 0.5391\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 58s - loss: 1.0026 - acc: 0.6252 - val_loss: 1.2255 - val_acc: 0.5500\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 58s - loss: 0.9115 - acc: 0.6583 - val_loss: 1.2033 - val_acc: 0.5531\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 56s - loss: 0.8206 - acc: 0.6929 - val_loss: 1.2590 - val_acc: 0.5667\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.7575 - acc: 0.7210 - val_loss: 1.2992 - val_acc: 0.5662\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.6553 - acc: 0.7563 - val_loss: 1.3472 - val_acc: 0.5762\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.5663 - acc: 0.7909 - val_loss: 1.4494 - val_acc: 0.5720\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.5202 - acc: 0.8098 - val_loss: 1.5311 - val_acc: 0.5784\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.4428 - acc: 0.8391 - val_loss: 1.6206 - val_acc: 0.5729\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.4157 - acc: 0.8492 - val_loss: 1.6929 - val_acc: 0.5690\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.3629 - acc: 0.8705 - val_loss: 1.6588 - val_acc: 0.5701\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.3086 - acc: 0.8922 - val_loss: 1.8304 - val_acc: 0.5832\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.2899 - acc: 0.8992 - val_loss: 1.9449 - val_acc: 0.5726\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.2594 - acc: 0.9094 - val_loss: 2.0884 - val_acc: 0.5709\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.2591 - acc: 0.9094 - val_loss: 2.1292 - val_acc: 0.5729\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.2066 - acc: 0.9289 - val_loss: 2.0931 - val_acc: 0.5795\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.2136 - acc: 0.9255 - val_loss: 2.1098 - val_acc: 0.5776\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1730 - acc: 0.9414 - val_loss: 2.2312 - val_acc: 0.5795\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1749 - acc: 0.9409 - val_loss: 2.1824 - val_acc: 0.5673\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1480 - acc: 0.9507 - val_loss: 2.1631 - val_acc: 0.5823\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1707 - acc: 0.9437 - val_loss: 2.2541 - val_acc: 0.5704\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1401 - acc: 0.9536 - val_loss: 2.3224 - val_acc: 0.5776\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1256 - acc: 0.9581 - val_loss: 2.3334 - val_acc: 0.5818\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1226 - acc: 0.9598 - val_loss: 2.3628 - val_acc: 0.5809\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1142 - acc: 0.9620 - val_loss: 2.4884 - val_acc: 0.5701\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1122 - acc: 0.9613 - val_loss: 2.4373 - val_acc: 0.5815\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1091 - acc: 0.9639 - val_loss: 2.3203 - val_acc: 0.5743\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1147 - acc: 0.9626 - val_loss: 2.2274 - val_acc: 0.5809\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1637 - acc: 0.9456 - val_loss: 2.2757 - val_acc: 0.5865\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1023 - acc: 0.9656 - val_loss: 2.3166 - val_acc: 0.5826\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1162 - acc: 0.9618 - val_loss: 2.3565 - val_acc: 0.5910\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1135 - acc: 0.9629 - val_loss: 2.3643 - val_acc: 0.5848\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0972 - acc: 0.9690 - val_loss: 2.4079 - val_acc: 0.5815\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1009 - acc: 0.9669 - val_loss: 2.4452 - val_acc: 0.5832\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0797 - acc: 0.9746 - val_loss: 2.3846 - val_acc: 0.5846\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1278 - acc: 0.9588 - val_loss: 2.4367 - val_acc: 0.5737\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1216 - acc: 0.9584 - val_loss: 2.4391 - val_acc: 0.5812\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0919 - acc: 0.9691 - val_loss: 2.4983 - val_acc: 0.5790\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0847 - acc: 0.9722 - val_loss: 2.4663 - val_acc: 0.5729\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0969 - acc: 0.9682 - val_loss: 2.4624 - val_acc: 0.5848\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0706 - acc: 0.9769 - val_loss: 2.5982 - val_acc: 0.5793\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0718 - acc: 0.9767 - val_loss: 2.4779 - val_acc: 0.5812\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0660 - acc: 0.9785 - val_loss: 2.7305 - val_acc: 0.5804\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1189 - acc: 0.9602 - val_loss: 2.2877 - val_acc: 0.5773\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.1128 - acc: 0.9633 - val_loss: 2.4842 - val_acc: 0.5837\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0797 - acc: 0.9741 - val_loss: 2.3556 - val_acc: 0.5801\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0767 - acc: 0.9756 - val_loss: 2.5308 - val_acc: 0.5768\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0975 - acc: 0.9689 - val_loss: 2.3872 - val_acc: 0.5740\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0753 - acc: 0.9750 - val_loss: 2.3587 - val_acc: 0.5723\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0915 - acc: 0.9698 - val_loss: 2.4329 - val_acc: 0.5773\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0715 - acc: 0.9772 - val_loss: 2.5353 - val_acc: 0.5913\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0812 - acc: 0.9729 - val_loss: 2.4956 - val_acc: 0.5887\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 55s - loss: 0.0920 - acc: 0.9699 - val_loss: 2.4285 - val_acc: 0.5896\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0910 - acc: 0.9698 - val_loss: 2.3188 - val_acc: 0.5893\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0633 - acc: 0.9797 - val_loss: 2.4646 - val_acc: 0.5840\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0742 - acc: 0.9751 - val_loss: 2.4274 - val_acc: 0.5874\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0642 - acc: 0.9799 - val_loss: 2.5481 - val_acc: 0.5795\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.1023 - acc: 0.9661 - val_loss: 2.4414 - val_acc: 0.5868\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0603 - acc: 0.9796 - val_loss: 2.5459 - val_acc: 0.5848\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0825 - acc: 0.9742 - val_loss: 2.5670 - val_acc: 0.5851\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0758 - acc: 0.9743 - val_loss: 2.5975 - val_acc: 0.5809\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0770 - acc: 0.9751 - val_loss: 2.6458 - val_acc: 0.5843\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0663 - acc: 0.9783 - val_loss: 2.6241 - val_acc: 0.5807\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0586 - acc: 0.9808 - val_loss: 2.5903 - val_acc: 0.5837\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0555 - acc: 0.9818 - val_loss: 2.5646 - val_acc: 0.5812\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0489 - acc: 0.9840 - val_loss: 2.7173 - val_acc: 0.5762\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0547 - acc: 0.9825 - val_loss: 2.6110 - val_acc: 0.5840\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0556 - acc: 0.9818 - val_loss: 2.7343 - val_acc: 0.5717\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0733 - acc: 0.9762 - val_loss: 2.7062 - val_acc: 0.5857\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0681 - acc: 0.9771 - val_loss: 2.5076 - val_acc: 0.5901\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0575 - acc: 0.9809 - val_loss: 2.7188 - val_acc: 0.5798\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0880 - acc: 0.9711 - val_loss: 2.6226 - val_acc: 0.5720\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.1053 - acc: 0.9662 - val_loss: 2.5095 - val_acc: 0.5795\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.1145 - acc: 0.9625 - val_loss: 2.4138 - val_acc: 0.5851\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0665 - acc: 0.9788 - val_loss: 2.4638 - val_acc: 0.5815\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0573 - acc: 0.9818 - val_loss: 2.4895 - val_acc: 0.5876\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0508 - acc: 0.9836 - val_loss: 2.4883 - val_acc: 0.5804\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0589 - acc: 0.9812 - val_loss: 2.6765 - val_acc: 0.5765\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0525 - acc: 0.9826 - val_loss: 2.6236 - val_acc: 0.5701\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0601 - acc: 0.9813 - val_loss: 2.5681 - val_acc: 0.5815\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0674 - acc: 0.9782 - val_loss: 2.6474 - val_acc: 0.5765\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0530 - acc: 0.9822 - val_loss: 2.6636 - val_acc: 0.5840\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0626 - acc: 0.9795 - val_loss: 2.8541 - val_acc: 0.5748\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0649 - acc: 0.9789 - val_loss: 2.5695 - val_acc: 0.5812\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0824 - acc: 0.9737 - val_loss: 2.4580 - val_acc: 0.5846\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0585 - acc: 0.9804 - val_loss: 2.6158 - val_acc: 0.5896\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0690 - acc: 0.9772 - val_loss: 2.5743 - val_acc: 0.5787\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0679 - acc: 0.9778 - val_loss: 2.7140 - val_acc: 0.5801\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0952 - acc: 0.9683 - val_loss: 2.5047 - val_acc: 0.5826\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0613 - acc: 0.9801 - val_loss: 2.5697 - val_acc: 0.5848\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0702 - acc: 0.9773 - val_loss: 2.5583 - val_acc: 0.5818\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0600 - acc: 0.9813 - val_loss: 2.5938 - val_acc: 0.5876\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0728 - acc: 0.9764 - val_loss: 2.5639 - val_acc: 0.5798\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0561 - acc: 0.9816 - val_loss: 2.5500 - val_acc: 0.5804\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0745 - acc: 0.9762 - val_loss: 2.6794 - val_acc: 0.5807\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0721 - acc: 0.9762 - val_loss: 2.5168 - val_acc: 0.5804\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0569 - acc: 0.9805 - val_loss: 2.6422 - val_acc: 0.5837\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0595 - acc: 0.9813 - val_loss: 2.5496 - val_acc: 0.5748\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0574 - acc: 0.9803 - val_loss: 2.6706 - val_acc: 0.5759\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0560 - acc: 0.9820 - val_loss: 2.5888 - val_acc: 0.5745\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.1001 - acc: 0.9681 - val_loss: 2.4206 - val_acc: 0.5779\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0983 - acc: 0.9680 - val_loss: 2.4264 - val_acc: 0.5737\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0580 - acc: 0.9811 - val_loss: 2.4190 - val_acc: 0.5848\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0579 - acc: 0.9815 - val_loss: 2.4199 - val_acc: 0.5756\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0750 - acc: 0.9755 - val_loss: 2.4093 - val_acc: 0.5809\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0504 - acc: 0.9839 - val_loss: 2.6697 - val_acc: 0.5784\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0535 - acc: 0.9838 - val_loss: 2.5750 - val_acc: 0.5756\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0496 - acc: 0.9847 - val_loss: 2.6025 - val_acc: 0.5807\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0506 - acc: 0.9836 - val_loss: 2.6215 - val_acc: 0.5807\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0427 - acc: 0.9857 - val_loss: 2.7617 - val_acc: 0.5848\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0661 - acc: 0.9796 - val_loss: 2.5334 - val_acc: 0.5874\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0502 - acc: 0.9842 - val_loss: 2.5125 - val_acc: 0.5784\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0550 - acc: 0.9827 - val_loss: 2.6483 - val_acc: 0.5790\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0494 - acc: 0.9847 - val_loss: 2.6424 - val_acc: 0.5801\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0638 - acc: 0.9794 - val_loss: 2.5024 - val_acc: 0.5798\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0501 - acc: 0.9830 - val_loss: 2.6842 - val_acc: 0.5798\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0406 - acc: 0.9862 - val_loss: 2.7987 - val_acc: 0.5823\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0507 - acc: 0.9837 - val_loss: 2.5790 - val_acc: 0.5740\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32298/32298 [==============================] - 53s - loss: 0.0440 - acc: 0.9856 - val_loss: 2.8633 - val_acc: 0.5745\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0501 - acc: 0.9838 - val_loss: 2.6201 - val_acc: 0.5762\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0580 - acc: 0.9812 - val_loss: 2.6114 - val_acc: 0.5717\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0523 - acc: 0.9835 - val_loss: 2.6493 - val_acc: 0.5773\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0566 - acc: 0.9813 - val_loss: 2.7453 - val_acc: 0.5812\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0574 - acc: 0.9820 - val_loss: 2.6001 - val_acc: 0.5773\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0535 - acc: 0.9828 - val_loss: 2.5778 - val_acc: 0.5879\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0534 - acc: 0.9831 - val_loss: 2.4427 - val_acc: 0.5731\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0650 - acc: 0.9792 - val_loss: 2.7663 - val_acc: 0.5734\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0508 - acc: 0.9843 - val_loss: 2.5061 - val_acc: 0.5782\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 54s - loss: 0.0505 - acc: 0.9839 - val_loss: 2.6322 - val_acc: 0.5874\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0964 - acc: 0.9694 - val_loss: 2.4846 - val_acc: 0.5798\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0774 - acc: 0.9740 - val_loss: 2.5400 - val_acc: 0.5756\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0530 - acc: 0.9823 - val_loss: 2.6008 - val_acc: 0.5790\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0471 - acc: 0.9851 - val_loss: 2.8561 - val_acc: 0.5743\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0827 - acc: 0.9750 - val_loss: 2.4467 - val_acc: 0.5793\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0505 - acc: 0.9841 - val_loss: 2.4962 - val_acc: 0.5795\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0436 - acc: 0.9859 - val_loss: 2.6678 - val_acc: 0.5751\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0442 - acc: 0.9854 - val_loss: 2.6017 - val_acc: 0.5801\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0488 - acc: 0.9844 - val_loss: 2.5152 - val_acc: 0.5665\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0837 - acc: 0.9733 - val_loss: 2.4218 - val_acc: 0.5745\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0557 - acc: 0.9824 - val_loss: 2.6077 - val_acc: 0.5748\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0588 - acc: 0.9823 - val_loss: 2.4994 - val_acc: 0.5857\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0415 - acc: 0.9872 - val_loss: 2.6981 - val_acc: 0.5790\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0390 - acc: 0.9871 - val_loss: 2.8047 - val_acc: 0.5748\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0882 - acc: 0.9726 - val_loss: 2.4802 - val_acc: 0.5768\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0627 - acc: 0.9802 - val_loss: 2.4055 - val_acc: 0.5762\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0772 - acc: 0.9754 - val_loss: 2.5536 - val_acc: 0.5779\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0557 - acc: 0.9827 - val_loss: 2.5508 - val_acc: 0.5834\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0476 - acc: 0.9850 - val_loss: 2.6315 - val_acc: 0.5832\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0424 - acc: 0.9866 - val_loss: 2.5477 - val_acc: 0.5793\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0463 - acc: 0.9853 - val_loss: 2.5848 - val_acc: 0.5790\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0436 - acc: 0.9859 - val_loss: 2.6095 - val_acc: 0.5770\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0745 - acc: 0.9765 - val_loss: 2.5509 - val_acc: 0.5793\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0470 - acc: 0.9852 - val_loss: 2.4078 - val_acc: 0.5765\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0496 - acc: 0.9838 - val_loss: 2.5612 - val_acc: 0.5773\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0497 - acc: 0.9848 - val_loss: 2.6246 - val_acc: 0.5793\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0476 - acc: 0.9850 - val_loss: 2.6666 - val_acc: 0.5712\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0361 - acc: 0.9886 - val_loss: 2.7086 - val_acc: 0.5759\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0376 - acc: 0.9877 - val_loss: 2.4981 - val_acc: 0.5793\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0419 - acc: 0.9865 - val_loss: 2.6809 - val_acc: 0.5770\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0478 - acc: 0.9857 - val_loss: 2.6668 - val_acc: 0.5740\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0398 - acc: 0.9872 - val_loss: 2.5516 - val_acc: 0.5795\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0385 - acc: 0.9881 - val_loss: 2.5803 - val_acc: 0.5754\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0509 - acc: 0.9835 - val_loss: 2.6813 - val_acc: 0.5695\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0417 - acc: 0.9872 - val_loss: 2.5344 - val_acc: 0.5787\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0687 - acc: 0.9783 - val_loss: 2.6035 - val_acc: 0.5821\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0503 - acc: 0.9853 - val_loss: 2.5415 - val_acc: 0.5748\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0591 - acc: 0.9811 - val_loss: 2.5365 - val_acc: 0.5765\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0467 - acc: 0.9839 - val_loss: 2.5419 - val_acc: 0.5770\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0422 - acc: 0.9866 - val_loss: 2.7550 - val_acc: 0.5667\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0544 - acc: 0.9833 - val_loss: 2.5317 - val_acc: 0.5812\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0388 - acc: 0.9871 - val_loss: 2.6087 - val_acc: 0.5770\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0431 - acc: 0.9866 - val_loss: 2.4521 - val_acc: 0.5743\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0355 - acc: 0.9890 - val_loss: 2.6521 - val_acc: 0.5804\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0626 - acc: 0.9812 - val_loss: 2.5991 - val_acc: 0.5815\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0486 - acc: 0.9847 - val_loss: 2.4410 - val_acc: 0.5957\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0429 - acc: 0.9860 - val_loss: 2.4074 - val_acc: 0.5759\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0492 - acc: 0.9848 - val_loss: 2.3722 - val_acc: 0.5924\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0627 - acc: 0.9794 - val_loss: 2.6743 - val_acc: 0.5782\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0914 - acc: 0.9724 - val_loss: 2.4440 - val_acc: 0.5776\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0518 - acc: 0.9838 - val_loss: 2.5029 - val_acc: 0.5734\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0544 - acc: 0.9835 - val_loss: 2.5338 - val_acc: 0.5821\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32298/32298 [==============================] - 53s - loss: 0.0528 - acc: 0.9833 - val_loss: 2.4688 - val_acc: 0.5790\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0386 - acc: 0.9871 - val_loss: 2.5995 - val_acc: 0.5737\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0456 - acc: 0.9862 - val_loss: 2.2867 - val_acc: 0.5804\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0350 - acc: 0.9894 - val_loss: 2.5229 - val_acc: 0.5756\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0373 - acc: 0.9882 - val_loss: 2.4276 - val_acc: 0.5770\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0425 - acc: 0.9863 - val_loss: 2.5885 - val_acc: 0.5770\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0462 - acc: 0.9857 - val_loss: 2.8204 - val_acc: 0.5740\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0444 - acc: 0.9858 - val_loss: 2.5466 - val_acc: 0.5720\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0554 - acc: 0.9834 - val_loss: 2.4797 - val_acc: 0.5804\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0479 - acc: 0.9847 - val_loss: 2.3906 - val_acc: 0.5756\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0554 - acc: 0.9829 - val_loss: 2.4170 - val_acc: 0.5762\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0517 - acc: 0.9839 - val_loss: 2.5601 - val_acc: 0.5648\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0511 - acc: 0.9846 - val_loss: 2.4023 - val_acc: 0.5784\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0534 - acc: 0.9834 - val_loss: 2.3593 - val_acc: 0.5857\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0394 - acc: 0.9876 - val_loss: 2.5285 - val_acc: 0.5743\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0324 - acc: 0.9889 - val_loss: 2.5801 - val_acc: 0.5812\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0378 - acc: 0.9882 - val_loss: 2.5112 - val_acc: 0.5698\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0446 - acc: 0.9854 - val_loss: 2.4134 - val_acc: 0.5754\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0375 - acc: 0.9882 - val_loss: 2.4061 - val_acc: 0.5637\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0373 - acc: 0.9884 - val_loss: 2.5168 - val_acc: 0.5729\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0607 - acc: 0.9805 - val_loss: 2.4037 - val_acc: 0.5762\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0499 - acc: 0.9841 - val_loss: 2.4606 - val_acc: 0.5765\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0457 - acc: 0.9859 - val_loss: 2.7663 - val_acc: 0.5773\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0423 - acc: 0.9873 - val_loss: 2.5446 - val_acc: 0.5795\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0374 - acc: 0.9887 - val_loss: 2.5927 - val_acc: 0.5759\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0432 - acc: 0.9862 - val_loss: 2.4924 - val_acc: 0.5812\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0343 - acc: 0.9884 - val_loss: 2.6458 - val_acc: 0.5812\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0427 - acc: 0.9867 - val_loss: 2.4516 - val_acc: 0.5857\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0384 - acc: 0.9882 - val_loss: 2.6249 - val_acc: 0.5782\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0478 - acc: 0.9859 - val_loss: 2.5856 - val_acc: 0.5840\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0647 - acc: 0.9804 - val_loss: 2.6542 - val_acc: 0.5743\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0498 - acc: 0.9845 - val_loss: 2.5750 - val_acc: 0.5720\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0510 - acc: 0.9839 - val_loss: 2.6366 - val_acc: 0.5754\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0619 - acc: 0.9814 - val_loss: 2.4375 - val_acc: 0.5782\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0447 - acc: 0.9863 - val_loss: 2.3982 - val_acc: 0.5770\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0354 - acc: 0.9884 - val_loss: 2.4248 - val_acc: 0.5818\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0333 - acc: 0.9891 - val_loss: 2.5767 - val_acc: 0.5795\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0384 - acc: 0.9881 - val_loss: 2.6276 - val_acc: 0.5756\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0389 - acc: 0.9880 - val_loss: 2.5264 - val_acc: 0.5795\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0427 - acc: 0.9867 - val_loss: 2.5376 - val_acc: 0.5759\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0347 - acc: 0.9892 - val_loss: 2.6326 - val_acc: 0.5770\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0403 - acc: 0.9874 - val_loss: 2.4052 - val_acc: 0.5698\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0363 - acc: 0.9874 - val_loss: 2.5641 - val_acc: 0.5829\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0528 - acc: 0.9843 - val_loss: 2.4800 - val_acc: 0.5765\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0638 - acc: 0.9801 - val_loss: 2.6727 - val_acc: 0.5729\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0646 - acc: 0.9816 - val_loss: 2.4746 - val_acc: 0.5723\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0427 - acc: 0.9865 - val_loss: 2.5793 - val_acc: 0.5731\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0483 - acc: 0.9854 - val_loss: 2.4542 - val_acc: 0.5793\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0695 - acc: 0.9799 - val_loss: 2.3977 - val_acc: 0.5765\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0521 - acc: 0.9840 - val_loss: 2.4811 - val_acc: 0.5784\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0389 - acc: 0.9876 - val_loss: 2.6416 - val_acc: 0.5740\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0392 - acc: 0.9874 - val_loss: 2.5159 - val_acc: 0.5790\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0379 - acc: 0.9885 - val_loss: 2.6155 - val_acc: 0.5809\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0370 - acc: 0.9883 - val_loss: 2.6656 - val_acc: 0.5737\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0482 - acc: 0.9846 - val_loss: 2.5120 - val_acc: 0.5784\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0459 - acc: 0.9857 - val_loss: 2.3511 - val_acc: 0.5651\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0640 - acc: 0.9815 - val_loss: 2.4472 - val_acc: 0.5773\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0611 - acc: 0.9820 - val_loss: 2.4636 - val_acc: 0.5807\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0409 - acc: 0.9871 - val_loss: 2.5357 - val_acc: 0.5843\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0429 - acc: 0.9865 - val_loss: 2.6374 - val_acc: 0.5821\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0402 - acc: 0.9874 - val_loss: 2.5063 - val_acc: 0.5826\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0818 - acc: 0.9752 - val_loss: 2.4535 - val_acc: 0.5773\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0460 - acc: 0.9861 - val_loss: 2.5365 - val_acc: 0.5748\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0446 - acc: 0.9869 - val_loss: 2.3372 - val_acc: 0.5751\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0404 - acc: 0.9882 - val_loss: 2.4146 - val_acc: 0.5843\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0345 - acc: 0.9887 - val_loss: 2.4157 - val_acc: 0.5754\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0354 - acc: 0.9892 - val_loss: 2.5209 - val_acc: 0.5854\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0321 - acc: 0.9902 - val_loss: 2.5515 - val_acc: 0.5876\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0357 - acc: 0.9894 - val_loss: 2.6602 - val_acc: 0.5946\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0392 - acc: 0.9876 - val_loss: 2.6095 - val_acc: 0.5868\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0410 - acc: 0.9874 - val_loss: 2.7849 - val_acc: 0.5784\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0493 - acc: 0.9857 - val_loss: 2.6183 - val_acc: 0.5762\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0404 - acc: 0.9869 - val_loss: 2.6717 - val_acc: 0.5784\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0772 - acc: 0.9772 - val_loss: 2.5039 - val_acc: 0.5665\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0456 - acc: 0.9860 - val_loss: 2.5174 - val_acc: 0.5809\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0478 - acc: 0.9843 - val_loss: 2.4751 - val_acc: 0.5770\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0530 - acc: 0.9840 - val_loss: 2.6599 - val_acc: 0.5818\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0806 - acc: 0.9767 - val_loss: 2.4842 - val_acc: 0.5826\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0409 - acc: 0.9872 - val_loss: 2.3934 - val_acc: 0.5904\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0322 - acc: 0.9897 - val_loss: 2.5926 - val_acc: 0.5818\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0303 - acc: 0.9907 - val_loss: 2.5432 - val_acc: 0.5812\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0377 - acc: 0.9877 - val_loss: 2.4013 - val_acc: 0.5801\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0407 - acc: 0.9879 - val_loss: 2.5285 - val_acc: 0.5823\n",
      "Epoch 2/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0455 - acc: 0.9859 - val_loss: 2.5233 - val_acc: 0.5899\n",
      "Epoch 3/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0329 - acc: 0.9896 - val_loss: 2.5109 - val_acc: 0.5770\n",
      "Epoch 4/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0639 - acc: 0.9809 - val_loss: 2.4328 - val_acc: 0.5756\n",
      "Epoch 5/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0417 - acc: 0.9876 - val_loss: 2.4729 - val_acc: 0.5832\n",
      "Epoch 6/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0447 - acc: 0.9870 - val_loss: 2.4876 - val_acc: 0.5876\n",
      "Epoch 7/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0426 - acc: 0.9870 - val_loss: 2.5946 - val_acc: 0.5720\n",
      "Epoch 8/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0429 - acc: 0.9868 - val_loss: 2.5159 - val_acc: 0.5798\n",
      "Epoch 9/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0422 - acc: 0.9877 - val_loss: 2.3552 - val_acc: 0.5768\n",
      "Epoch 10/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0371 - acc: 0.9887 - val_loss: 2.6243 - val_acc: 0.5862\n",
      "Epoch 11/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0362 - acc: 0.9889 - val_loss: 2.5394 - val_acc: 0.5829\n",
      "Epoch 12/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0351 - acc: 0.9897 - val_loss: 2.3580 - val_acc: 0.5837\n",
      "Epoch 13/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0351 - acc: 0.9894 - val_loss: 2.4088 - val_acc: 0.5890\n",
      "Epoch 14/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0350 - acc: 0.9896 - val_loss: 2.7156 - val_acc: 0.5756\n",
      "Epoch 15/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0592 - acc: 0.9820 - val_loss: 2.5167 - val_acc: 0.5801\n",
      "Epoch 16/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0537 - acc: 0.9838 - val_loss: 2.3815 - val_acc: 0.5809\n",
      "Epoch 17/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0797 - acc: 0.9767 - val_loss: 2.2630 - val_acc: 0.5807\n",
      "Epoch 18/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0390 - acc: 0.9880 - val_loss: 2.5113 - val_acc: 0.5779\n",
      "Epoch 19/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0325 - acc: 0.9897 - val_loss: 2.3612 - val_acc: 0.5745\n",
      "Epoch 20/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0303 - acc: 0.9908 - val_loss: 2.5261 - val_acc: 0.5787\n",
      "Epoch 21/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0324 - acc: 0.9908 - val_loss: 2.4595 - val_acc: 0.5770\n",
      "Epoch 22/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0317 - acc: 0.9906 - val_loss: 2.6069 - val_acc: 0.5776\n",
      "Epoch 23/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0360 - acc: 0.9894 - val_loss: 2.5378 - val_acc: 0.5765\n",
      "Epoch 24/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0329 - acc: 0.9897 - val_loss: 2.6767 - val_acc: 0.5776\n",
      "Epoch 25/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0335 - acc: 0.9897 - val_loss: 2.5294 - val_acc: 0.5748\n",
      "Epoch 26/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0385 - acc: 0.9884 - val_loss: 2.5847 - val_acc: 0.5868\n",
      "Epoch 27/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0313 - acc: 0.9903 - val_loss: 2.4603 - val_acc: 0.5790\n",
      "Epoch 28/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0402 - acc: 0.9876 - val_loss: 2.6717 - val_acc: 0.5901\n",
      "Epoch 29/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0340 - acc: 0.9898 - val_loss: 2.5231 - val_acc: 0.5762\n",
      "Epoch 30/30\n",
      "32298/32298 [==============================] - 53s - loss: 0.0445 - acc: 0.9869 - val_loss: 2.3364 - val_acc: 0.5834\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    model.fit(data, labelsOneHot, validation_split = 0.1, batch_size = batchSize, epochs = epochs, verbose = 1)\n",
    "    model.save(os.getcwd() + '/saved_model/keras_face_expres_model_new' + str(i*epochs + epochs) +'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(os.getcwd() + '/saved_model/keras_face_expres_model40.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
